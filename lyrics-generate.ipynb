{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# Character level lyrics generation using RNNs (LSTM)\n",
    "import sys, os, random, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import CharlyricsDataset\n",
    "from RNN import RNN\n",
    "import glob\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "from pathlib import Path\n",
    "from config import config\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharlyricsDataset.CharLyricsDataset(config.DATA.LYRICS, config.TRAIN.MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1836262"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.TRAIN.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "612087"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(utils.get_total_characters(), config.TRAIN.HIDDEN_SIZE, config.TRAIN.LSTM_N_LAYERS, utils.get_total_characters()).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.TRAIN.LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1: Total Loss 148.56669910971934\nEpoch 2: Total Loss 122.61308879733086\nEpoch 3: Total Loss 114.45420882454285\nEpoch 4: Total Loss 111.89200695129541\nEpoch 5: Total Loss 109.1643524980545\nEpoch 6: Total Loss 109.25127456839267\nEpoch 7: Total Loss 108.76884311951123\nEpoch 8: Total Loss 108.44116926284937\nEpoch 9: Total Loss 108.30524064678413\nEpoch 10: Total Loss 108.12335863553561\n"
    }
   ],
   "source": [
    "for epoch in range(config.TRAIN.EPOCHS):\n",
    "    model.train()\n",
    "    # tq = tqdm(train_loader, total=len(train_loader), desc=f\"Training: Epoch {epoch+1}/{config.TRAIN.EPOCHS}\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        input_seq, output_seq = batch\n",
    "\n",
    "        input_seq = input_seq.to(device)\n",
    "        output_seq = output_seq.to(device)\n",
    "        loss = 0\n",
    "\n",
    "        # vectorize this\n",
    "        for c in range(config.TRAIN.MAX_LEN):\n",
    "            output = model(input_seq[:, c])\n",
    "            loss += loss_fn(output, output_seq[:, c])\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # tq.set_postfix(loss=loss.item())\n",
    "        # batch-gradient-descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Total Loss {total_loss/(config.TRAIN.MAX_LEN * len(train_loader))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prime=\"B\", total_len=300, temp=0.85):\n",
    "    generated_text = prime\n",
    "    last_char = prime\n",
    "    \n",
    "    for c in range(total_len):\n",
    "        input_char = torch.LongTensor(utils.char_to_label(last_char)).to(device)\n",
    "        out = model(input_char)\n",
    "        top_char = np.argmax(out.detach().cpu())\n",
    "        predicted = string.printable[top_char]\n",
    "        generated_text += predicted\n",
    "        last_char = predicted\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'be in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in i'"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "generate(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}